<html><head>
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<META HTTP-EQUIV="Content-Language" CONTENT="en-US"><META NAME="robots" CONTENT="index, follow">
<META NAME="rating" CONTENT="General"><META NAME="distribution" CONTENT="Global">
<META NAME="revisit-after" CONTENT="15 days"><META NAME="author" CONTENT="Laurent Itti, JeVois">
<META NAME="description" CONTENT="JeVois Smart Embedded Machine Vision Toolkit - module PythonObject6D">
<link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
<link rel='stylesheet prefetch' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.4.0/css/font-awesome.min.css'>
<link rel="stylesheet" type="text/css" href="/modstyle.css">
</head> <body>
<table class=modinfo><tr><td>
<table class=modinfotop><tr><td><a href="/moddoc/PythonObject6D/modinfo.html"><img src="/moddoc/PythonObject6D/icon.png" width=48></a></td>
<td valign=middle><table><tr><td class=modinfoname>Python Object 6D</td></tr>
<tr><td class=modinfosynopsis>Simple example of object detection using ORB keypoints followed by 6D pose estimation in Python. </td></tr></table></td></tr></table></td></tr>
<tr><td width=100%><table class=modinfoauth width=100%><tr><td>By Laurent Itti</td><td align=center>itti@usc.edu</td><td align=center>http://jevois.org</td><td align=right>GPL v3</td></tr></table></td></tr>
<tr><td><table class=videomapping><tr><td class=videomapping>
<table class=moduledata><tr><td class=moduledata>&nbsp;Language:&nbsp;Python</td><td class=moduledata align=center>Supports mappings with USB output:&nbsp;Yes</td><td class=moduledata align=right>Supports mappings with NO USB output:&nbsp;Yes&nbsp;</td></tr></table>
</td></tr>
<tr><td class=videomapping><small><b>&nbsp;Video Mapping: &nbsp; </b></small><tt>YUYV&nbsp;320&nbsp;262&nbsp;15.0&nbsp;YUYV&nbsp;320&nbsp;240&nbsp;15.0&nbsp;JeVois&nbsp;PythonObject6D</tt></td></tr>
</table></td></tr>
<tr><td><div class=container>
<div class=galleryItem><a href="screenshot1.png"><img src="screenshot1.png"></a></div>
<div class=galleryItem><a href="screenshot2.png"><img src="screenshot2.png"></a></div>
<div class=galleryItem><a href="screenshot3.png"><img src="screenshot3.png"></a></div>
<div class=galleryItem><a href="screenshot4.png"><img src="screenshot4.png"></a></div>
</div></td></tr><tr><td class=modinfodesc><h2>Module Documentation</h2><div class="textblock"><p>This module implements an object detector using ORB keypoints using OpenCV in Python. Its main goal is to also demonstrate full 6D pose recovery of the detected object, in Python, as well as locating in 3D a sub-element of the detected object (here, a window within a larger textured wall). See <a href='/moddoc/ObjectDetect/modinfo.html'><strong class='jvmod'>ObjectDetect</strong></a> for more info about object detection using keypoints. This module is available with <strong class='jvversion'>JeVois v1.6.3</strong> and later.</p>
<p>The algorithm consists of 5 phases:</p><ul>
<li>detect keypoint locations, typically corners or other distinctive texture elements or markings;</li>
<li>compute keypoint descriptors, which are summary representations of the image neighborhood around each keypoint;</li>
<li>match descriptors from current image to descriptors previously extracted from training images;</li>
<li>if enough matches are found between the current image and a given training image, and they are of good enough quality, compute the homography (geometric transformation) between keypoint locations in that training image and locations of the matching keypoints in the current image. If it is well conditioned (i.e., a 3D viewpoint change could well explain how the keypoints moved between the training and current images), declare that a match was found, and draw a pink rectangle around the detected whole object.</li>
<li>finally perform 6D pose estimation (3D translation + 3D rotation), here for a window located at a specific position within the whole object, given the known physical sizes of both the whole object and the window within. A green parallelepiped is drawn at that window's location, sinking into the whole object (as it is representing a tunnel or port into the object).</li>
</ul>
<p>For more information about ORB keypoint detection and matching in OpenCV, see, e.g., <a href="https://docs.opencv.org/3.4.0/d1/d89/tutorial_py_orb.html">https://docs.opencv.org/3.4.0/d1/d89/tutorial_py_orb.html</a></p>
<p>This module is provided for inspiration. It has no pretension of actually solving the FIRST Robotics Power Up (sm) vision problem in a complete and reliable way. It is released in the hope that FRC teams will try it out and get inspired to develop something much better for their own robot.</p>
<p>Note how, contrary to <a href='/moddoc/FirstVision/modinfo.html'><strong class='jvmod'>FirstVision</strong></a>, <a href='/moddoc/DemoArUco/modinfo.html'><strong class='jvmod'>DemoArUco</strong></a>, etc, the green parallelepiped is drawn going into the object instead of sticking out of it, as it is depicting a tunnel at the window location.</p>
<h2>Using this module </h2>
<p>This module is for now specific to the "exchange" of the FIRST Robotics 2018 Power Up (sm) challenge. See <a href="https://www.firstinspires.org/resource-library/frc/competition-manual-qa-system">https://www.firstinspires.org/resource-library/frc/competition-manual-qa-system</a></p>
<p>The exchange is a large textured structure with a window at the bottom into which robots should deliver foam cubes.</p>
<p>A reference picture of the whole exchange (taken from the official rules) is in <b>JEVOIS:/modules/JeVois/PythonObject6D/images/reference.png</b> on your JeVois microSD card. It will be processed when the module starts. No additional training procedure is needed.</p>
<p>If you change the reference image, you should also edit:</p><ul>
<li>values of <code>self.owm</code> and <code>self.ohm</code> to the width ahd height, in meters, of the actual physical object in your picture. Square pixels are assumed, so make sure the aspect ratio of your PNG image matches the aspect ratio in meters given by variables <code>self.owm</code> and <code>self.ohm</code> in the code.</li>
<li>values of <code>self.wintop</code>, <code>self.winleft</code>, <code>self.winw</code>, <code>self.winh</code> to the location of the top-left corner, in meters and relative to the top-left corner of the whole reference object, of a window of interest (the tunnel into which the cubes should be delivered), and width and height, in meters, of the window.</li>
</ul>
<p><b>TODO:</b> Add support for multiple images and online training as in <a href='/moddoc/ObjectDetect/modinfo.html'><strong class='jvmod'>ObjectDetect</strong></a></p>
<h2>Things to tinker with </h2>
<p>There are a number of limitations and caveats to this module:</p>
<ul>
<li>It does not use color, the input image is converted to grayscale before processing. One could use a different approach to object detection that would make use of color.</li>
<li>Results are often quite noisy. Maybe using another detector, like SIFT which provides subpixel accuracy, and better pruning of false matches (e.g., David Lowe's ratio of the best to second-best match scores) would help.</li>
<li>This algorithm is slow in this single-threaded Python example, and frame rate depends on image complexity (it gets slower when more keypoints are detected). One should explore parallelization, as was done in C++ for the  <a href='/moddoc/ObjectDetect/modinfo.html'><strong class='jvmod'>ObjectDetect</strong></a> module. One could also alternate between full detection using this algorithm once in a while, and much faster tracking of previous detections at a higher framerate (e.g., using the very robust TLD tracker (track-learn-detect), also supported in OpenCV).</li>
<li>If you want to detect smaller objects or pieces of objects, and you do not need 6D pose, you may want to use modules  <a href='/moddoc/ObjectDetect/modinfo.html'><strong class='jvmod'>ObjectDetect</strong></a> or <a href='/moddoc/SaliencySURF/modinfo.html'><strong class='jvmod'>SaliencySURF</strong></a> as done, for example, by JeVois user Bill Kendall at <a href="https://www.youtube.com/watch?v=8wYhOnsNZcc">https://www.youtube.com/watch?v=8wYhOnsNZcc</a></li>
</ul>
</div></td></tr>
<tr><td><table class=modinfopar><tr><th class=modinfopar>Parameter</th><th class=modinfopar>Type</th><th class=modinfopar>Description</th><th class=modinfopar>Default</th><th class=modinfopar>Valid&nbsp;Values</th></tr>
<tr><td colspan=5>This module exposes no parameter</td></tr>
</table></td></tr>
<tr><td><table class=modinfomisc>
<tr class=modinfomisc><th class=modinfomisc>Detailed docs:</th><td class=modinfomisc><A HREF="/basedoc/classPythonObject6D_1_1PythonObject6D.html">PythonObject6D</A></td></tr>
<tr class=modinfomisc><th class=modinfomisc>Copyright:</th><td class=modinfomisc>Copyright (C) 2018 by Laurent Itti, iLab and the University of Southern California</td></tr>
<tr class=modinfomisc><th class=modinfomisc>License:</th><td class=modinfomisc>GPL v3</td></tr>
<tr class=modinfomisc><th class=modinfomisc>Distribution:</th><td class=modinfomisc>Unrestricted</td></tr>
<tr class=modinfomisc><th class=modinfomisc>Restrictions:</th><td class=modinfomisc>None</td></tr>
<tr class=modinfomisc><th class=modinfomisc>Support URL:</th><td class=modinfomisc>http://jevois.org/doc</td></tr>
<tr class=modinfomisc><th class=modinfomisc>Other URL:</th><td class=modinfomisc>http://iLab.usc.edu</td></tr>
<tr class=modinfomisc><th class=modinfomisc>Address:</th><td class=modinfomisc>University of Southern California, HNB-07A, 3641 Watt Way, Los Angeles, CA 90089-2520, USA</td></tr>
</table></td></tr>
</table>
</body></html>
